#!/usr/bin/env python3
"""
Webã‚µã‚¤ãƒˆã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Usage: python web_scraper_sitemap.py <URL>
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import sys
from collections import defaultdict
import json

class SitemapGenerator:
    def __init__(self, base_url, max_depth=3):
        self.base_url = base_url
        self.max_depth = max_depth
        self.visited = set()
        self.sitemap = defaultdict(list)
        self.domain = urlparse(base_url).netloc

    def is_valid_url(self, url):
        """åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã®URLã‹ãƒã‚§ãƒƒã‚¯"""
        parsed = urlparse(url)
        return parsed.netloc == self.domain and parsed.scheme in ['http', 'https']

    def normalize_url(self, url):
        """URLã‚’æ­£è¦åŒ–ï¼ˆãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆå‰Šé™¤ãªã©ï¼‰"""
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    def scrape_page(self, url, depth=0):
        """ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        if depth > self.max_depth or url in self.visited:
            return

        self.visited.add(url)
        print(f"{'  ' * depth}Scraping: {url}")

        try:
            # SSLè¨¼æ˜æ›¸ã®æ¤œè¨¼ã‚’ç„¡åŠ¹åŒ–ï¼ˆè‡ªå·±ç½²åè¨¼æ˜æ›¸å¯¾å¿œï¼‰
            response = requests.get(url, timeout=10, verify=False,
                                   headers={'User-Agent': 'Mozilla/5.0'})
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title = soup.find('title')
            title_text = title.get_text().strip() if title else 'No Title'

            # ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’ä¿å­˜
            self.sitemap[url] = {
                'title': title_text,
                'depth': depth,
                'links': []
            }

            # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            links = soup.find_all('a', href=True)

            for link in links:
                href = link.get('href')
                absolute_url = urljoin(url, href)
                normalized_url = self.normalize_url(absolute_url)

                if self.is_valid_url(normalized_url) and normalized_url not in self.visited:
                    link_text = link.get_text().strip()
                    self.sitemap[url]['links'].append({
                        'url': normalized_url,
                        'text': link_text
                    })

                    # å†å¸°çš„ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                    if depth < self.max_depth:
                        self.scrape_page(normalized_url, depth + 1)

        except requests.exceptions.SSLError as e:
            print(f"  SSL Error on {url}: {e}")
        except requests.exceptions.RequestException as e:
            print(f"  Error scraping {url}: {e}")
        except Exception as e:
            print(f"  Unexpected error on {url}: {e}")

    def generate_markdown_sitemap(self, output_file='sitemap.md'):
        """Markdownå½¢å¼ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# ã‚µã‚¤ãƒˆãƒãƒƒãƒ—: {self.base_url}\n\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {self.get_timestamp()}\n\n")
            f.write(f"## ç™ºè¦‹ã•ã‚ŒãŸãƒšãƒ¼ã‚¸æ•°: {len(self.visited)}\n\n")
            f.write("---\n\n")

            # æ·±ã•ã”ã¨ã«æ•´ç†
            by_depth = defaultdict(list)
            for url, info in self.sitemap.items():
                by_depth[info['depth']].append((url, info))

            for depth in sorted(by_depth.keys()):
                f.write(f"## ãƒ¬ãƒ™ãƒ« {depth}\n\n")
                for url, info in sorted(by_depth[depth], key=lambda x: x[0]):
                    indent = "  " * depth
                    f.write(f"{indent}- **{info['title']}**\n")
                    f.write(f"{indent}  - URL: `{url}`\n")
                    if info['links']:
                        f.write(f"{indent}  - å­ãƒšãƒ¼ã‚¸ ({len(info['links'])}):\n")
                        for link in info['links'][:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
                            f.write(f"{indent}    - [{link['text']}]({link['url']})\n")
                    f.write("\n")

        print(f"\nâœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_file}")

    def generate_tree_view(self):
        """ãƒ„ãƒªãƒ¼å½¢å¼ã§ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ã‚µã‚¤ãƒˆãƒãƒƒãƒ—æ§‹é€ :")
        print("="*80 + "\n")

        by_depth = defaultdict(list)
        for url, info in self.sitemap.items():
            by_depth[info['depth']].append((url, info))

        for depth in sorted(by_depth.keys()):
            for url, info in sorted(by_depth[depth], key=lambda x: x[0]):
                indent = "  " * depth
                print(f"{indent}{'â””â”€ ' if depth > 0 else ''}ğŸ“„ {info['title']}")
                print(f"{indent}   {url}")
                if info['links']:
                    print(f"{indent}   ({len(info['links'])} ãƒªãƒ³ã‚¯)")

    def generate_json_sitemap(self, output_file='sitemap.json'):
        """JSONå½¢å¼ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dict(self.sitemap), f, ensure_ascii=False, indent=2)
        print(f"âœ… JSONã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_file}")

    @staticmethod
    def get_timestamp():
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def main():
    if len(sys.argv) < 2:
        print("Usage: python web_scraper_sitemap.py <URL> [max_depth]")
        print("Example: python web_scraper_sitemap.py http://example.com 2")
        sys.exit(1)

    url = sys.argv[1]
    max_depth = int(sys.argv[2]) if len(sys.argv) > 2 else 2

    print(f"ğŸ” ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆä¸­: {url}")
    print(f"ğŸ“Š æœ€å¤§æ·±åº¦: {max_depth}\n")

    # SSLè­¦å‘Šã‚’æŠ‘åˆ¶
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    generator = SitemapGenerator(url, max_depth=max_depth)
    generator.scrape_page(url)

    # çµæœã‚’å‡ºåŠ›
    generator.generate_tree_view()
    generator.generate_markdown_sitemap()
    generator.generate_json_sitemap()

    print(f"\nâœ¨ å®Œäº†! {len(generator.visited)} ãƒšãƒ¼ã‚¸ã‚’è§£æã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()
